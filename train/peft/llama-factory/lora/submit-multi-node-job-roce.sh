#!/bin/bash
TS=$(date -u '+%Y%m%d%H%M%S')

export JOB_NAME=${JOB_NAME:-sample-torch-job-$TS}
export NNODES=${NNODES:-2}
export WORKER_NODES=$((NNODES - 1))
export MODEL_NAME=${MODEL_NAME:-meta-llama/Meta-Llama-3.1-8B-Instruct}
export GPUS_PER_NODE=${GPUS_PER_NODE:-2}
export GPUS_INDICES=$(seq -s, 0 $((GPUS_PER_NODE-1)) | sed 's/,$//')
let DEFAULT_CPU_RESOURCES_REQS=$((8 * ${GPUS_PER_NODE}))
let DEFAULT_CPU_RESOURCES_LIMITS=$((10 * ${GPUS_PER_NODE}))
let DEFAULT_MEM_RESOURCES_REQS=$((16 * ${GPUS_PER_NODE}))
let DEFAULT_MEM_RESOURCES_LIMITS=$((32 * ${GPUS_PER_NODE}))
export EPOCHS=${EPOCHS:-2}
export TRAIN_BATCH_SIZE_PER_DEVICE=${TRAIN_BATCH_SIZE_PER_DEVICE:-2}
export DATASET_TEMPLATE=${DATASET_TEMPLATE:-llama3}
export REPORT_TO=${REPORT_TO:-tensorboard}
export LOGGING_DIR=${LOGGING_DIR:-/app/output/logs}
export HCA_SHARED_DEVICES=${GPUS_PER_NODE}
export CPU_RESOURCES_REQS=${CPU_RESOURCES_REQS:-${DEFAULT_CPU_RESOURCES_REQS}}
export CPU_RESOURCES_LIMITS=${CPU_RESOURCES_LIMITS:-${DEFAULT_CPU_RESOURCES_LIMITS}}
export MEM_RESOURCES_REQS=${MEM_RESOURCES_REQS_:-${DEFAULT_MEM_RESOURCES_REQS}Gi}
export MEM_RESOURCES_LIMITS=${MEM_RESOURCES_LIMITS:-${DEFAULT_MEM_RESOURCES_LIMITS}Gi}
export GPU_SPEC=${GPU_SPEC:-A800_NVLINK_80GB}
export IMAGE_NAME=${IMAGE_NAME:-"10.5.1.249/bob-base-image/lora-peft:cuda-12.1.1-cudnn8-devel-ubuntu22.04-llama-factory-main-pytorch-24.02-py3-deepspeed"}
export NCCL_IB_DISABLE=${NCCL_IB_DISABLE:-0}
export NCCL_NET=${NCCL_NET:-IB}
export MASTER_PORT=${MASTER_PORT:-23456}
export NCCL_DEBUG=${NCCL_DEBUG:-INFO}
export NCCL_SOCKET_IFNAME=${NCCL_SOCKET_IFNAME:-bond0}

envsubst '$NCCL_NET $GPUS_INDICES $NCCL_SOCKET_IFNAME $MASTER_PORT $NCCL_IB_DISABLE $NCCL_DEBUG $JOB_NAME $MODEL_NAME $NNODES $WORKER_NODES $GPUS_PER_NODE $EPOCHS $TRAIN_BATCH_SIZE_PER_DEVICE $DATASET_TEMPLATE $REPORT_TO $LOGGING_DIR $GPU_SPEC $HCA_SHARED_DEVICES $CPU_RESOURCES_REQS $MEM_RESOURCES_REQS $CPU_RESOURCES_LIMITS $MEM_RESOURCES_LIMITS $IMAGE_NAME' < lora-example-pytorchjob-roce.tpl.yaml | kubectl replace --force -f -


